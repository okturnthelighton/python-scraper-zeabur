# å°å…¥æ‰€æœ‰å¿…è¦çš„å‡½å¼åº«
import os
import requests
import json
import urllib.parse
import time
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException, WebDriverException

# --- LINE ç›¸é—œè³‡è¨Š ---
# å¾æ‚¨åœ¨ Zeabur è¨­å®šçš„ç’°å¢ƒè®Šæ•¸ä¸­è®€å–
LINE_ACCESS_TOKEN = os.environ.get("LINE_ACCESS_TOKEN")
LINE_USER_ID = os.environ.get("LINE_USER_ID")
# --- çµæŸ LINE è³‡è¨Šéƒ¨åˆ† ---

def shorten_url(long_url):
    """ä½¿ç”¨ TinyURL API ç¸®çŸ­ç¶²å€"""
    if not long_url or long_url.strip() == "" or long_url == "æ²’æœ‰æ‰¾åˆ°ç›¸é—œé€£çµ":
        return "æ²’æœ‰æ‰¾åˆ°ç›¸é—œé€£çµ"
    # ä½¿ç”¨ safe='' ä¾†ç¢ºä¿æ•´å€‹ URL è¢«ç·¨ç¢¼
    api_url = f"https://tinyurl.com/api-create.php?url={urllib.parse.quote(long_url, safe='')}"
    try:
        # å¢åŠ è«‹æ±‚è¶…æ™‚è¨­å®š
        response = requests.get(api_url, timeout=10)
        # æª¢æŸ¥ç‹€æ…‹ç¢¼ï¼Œç¢ºä¿è«‹æ±‚æˆåŠŸ
        return response.text if response.status_code == 200 else long_url
    except requests.exceptions.RequestException as e:
        print(f"  > ç¸®çŸ­ç¶²å€å¤±æ•—: {e}")
        return long_url

def get_news_title_from_url(driver, url):
    """
    è¨ªå•æ–°èé€£çµä¸¦æå–æ¨™é¡Œã€‚
    """
    if not url or url == "æ²’æœ‰æ‰¾åˆ°ç›¸é—œé€£çµ" or url.startswith("https://www.google.com/search?q="):
        return "æ¨™é¡Œè§£æå¤±æ•—æˆ–ç‚ºå‚™ç”¨é€£çµ"
    try:
        print(f"  > æ­£åœ¨è¨ªå•æ–°èé€£çµä»¥ç²å–æ¨™é¡Œ: {url}")
        driver.get(url)
        # ç­‰å¾… title æ¨™ç±¤å‡ºç¾ï¼Œå¢åŠ ç©©å®šæ€§
        title_element = WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, "title")))
        page_title = title_element.get_attribute("textContent").strip()
        if page_title:
            print(f"  > æˆåŠŸå¾ <title> ç²å–æ¨™é¡Œ: {page_title}")
            return page_title
        return "æ¨™é¡Œæœªæ‰¾åˆ°"
    except TimeoutException:
        print(f"  > è¨ªå•æ–°èé€£çµè¶…æ™‚: {url}")
        return "æ¨™é¡Œè§£æè¶…æ™‚"
    except Exception as e:
        print(f"  > è¨ªå•æ–°èé€£çµæˆ–è§£ææ¨™é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return "æ¨™é¡Œè§£æå¤±æ•—"

def get_google_trends_data(driver):
    """
    çˆ¬å– Google Trends çš„ç„¦é»æ–°èèˆ‡æ‰€æœ‰ç†±é–€é—œéµå­—ã€‚
    """
    detailed_trends = [] # å„²å­˜ (keyword, link, title)
    all_keywords = [] # å„²å­˜æ‰€æœ‰ keyword

    initial_url = "https://trends.google.com.tw/trending?geo=TW"
    print("æ­£åœ¨è¼‰å…¥ Google Trends é¦–é ...")
    driver.get(initial_url)
    
    try:
        print("ç­‰å¾…ç†±é–€é—œéµå­—åˆ—è¡¨æ¸²æŸ“...")
        # ç­‰å¾…åˆ—è¡¨å®¹å™¨å‡ºç¾
        WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.CLASS_NAME, "feed-list-wrapper")))
        print("ç†±é–€é—œéµå­—åˆ—è¡¨å…ƒç´ å·²è¼‰å…¥ã€‚")
    except TimeoutException:
        print("âš ï¸ ç­‰å¾…é¦–é é—œéµå­—åˆ—è¡¨è¶…æ™‚ã€‚ç¨‹å¼ç„¡æ³•ç¹¼çºŒã€‚")
        return [], []

    # --- é¦–å…ˆï¼Œç²å–æ‰€æœ‰é—œéµå­— (ä½¿ç”¨ BeautifulSoup) ---
    try:
        soup = BeautifulSoup(driver.page_source, "html5lib")
        # Google Trends çš„ class åç¨±å¯èƒ½æœƒè®Šï¼Œä½¿ç”¨æ›´é€šç”¨çš„é¸æ“‡å™¨
        trend_elements = soup.find_all("div", class_="mZ3RIc")
        if trend_elements:
            print("\n--- æ­£åœ¨æŠ“å–æ‰€æœ‰ç†±é–€é—œéµå­— ---")
            for element in trend_elements:
                keyword = element.text.strip()
                if keyword:
                    all_keywords.append(keyword)
            print(f"æˆåŠŸæŠ“å–åˆ° {len(all_keywords)} å€‹é—œéµå­—ã€‚")
        else:
            print("â„¹ï¸ æœªèƒ½æŠ“å–åˆ°æ‰€æœ‰é—œéµå­—çš„åˆ—è¡¨ã€‚")
    except Exception as e:
        print(f"âŒ æŠ“å–æ‰€æœ‰é—œéµå­—åˆ—è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    # --- æ¥è‘—ï¼Œé€ä¸€è™•ç†ç„¦é»æ–°è ---
    print("\n--- æ­£åœ¨é€ä¸€æŠ“å–ç„¦é»æ–°èè©³æƒ… ---")
    try:
        rows_xpath = '//tr[contains(@class, "UlR2Yc") and @data-row-id]'
        rows = driver.find_elements(By.XPATH, rows_xpath)
        num_rows = len(rows)
        print(f"æ‰¾åˆ° {num_rows} å€‹ç„¦é»æ–°èã€‚")
    except NoSuchElementException:
        print("â„¹ï¸ åœ¨é é¢ä¸Šæ‰¾ä¸åˆ°ä»»ä½•ç„¦é»æ–°èåˆ—ã€‚")
        return [], all_keywords # å³ä½¿æ²’æœ‰ç„¦é»æ–°èï¼Œä¹Ÿè¦å›å‚³å·²æŠ“åˆ°çš„æ‰€æœ‰é—œéµå­—

    for i in range(num_rows):
        keyword = f"æœªçŸ¥(ç´¢å¼•{i})" # é è¨­å€¼
        try:
            # æ¯æ¬¡å¾ªç’°éƒ½é‡æ–°æŸ¥æ‰¾å…ƒç´ ï¼Œé¿å… StaleElementReferenceException
            current_rows = driver.find_elements(By.XPATH, rows_xpath)
            if i >= len(current_rows):
                print(f"è­¦å‘Šï¼šå…ƒç´ æ•¸é‡åœ¨è™•ç†éç¨‹ä¸­æ¸›å°‘ï¼Œæå‰çµæŸã€‚")
                break
            
            row = current_rows[i]
            keyword_element = row.find_element(By.CLASS_NAME, "mZ3RIc")
            keyword = keyword_element.text.strip()
            print(f"\n- æ­£åœ¨è™•ç†ç¬¬ {i+1}/{num_rows} å‰‡ç„¦é»æ–°è: {keyword}")

            # é»æ“Šé€²å…¥è©³ç´°é é¢
            driver.execute_script("arguments[0].click();", keyword_element)
            
            news_link = "æ²’æœ‰æ‰¾åˆ°ç›¸é—œé€£çµ"
            news_title = "æ¨™é¡Œæœªæ‰¾åˆ°"
            
            try:
                # ç­‰å¾…è©³ç´°é é¢çš„æ–°èé€£çµå‡ºç¾
                news_link_xpath = '//div[@class="jDtQ5"]//a[@class="xZCHj"]'
                news_link_element = WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, news_link_xpath)))
                news_link = news_link_element.get_attribute('href')
                print(f"  âœ¨ æ‰¾åˆ°é€£çµ: {news_link}")
                news_title = get_news_title_from_url(driver, news_link)
            except TimeoutException:
                print("  â„¹ï¸ åœ¨è©³ç´°é é¢æ²’æœ‰æ‰¾åˆ°ä¸»è¦æ–°èé€£çµï¼Œå°‡ä½¿ç”¨å‚™ç”¨æœå°‹é€£çµã€‚")
                news_link = f"https://www.google.com/search?q={urllib.parse.quote(keyword)}"
                news_title = "æœå°‹çµæœ (å‚™ç”¨é€£çµ)"

            detailed_trends.append((keyword, news_link, news_title))
            
            # [å„ªåŒ–] å®Œæˆå¾Œï¼Œå¿…é ˆè¿”å› Google Trends é¦–é ä¸¦ç­‰å¾…å…¶å¯äº’å‹•
            print("  > è™•ç†å®Œæˆï¼Œè¿”å› Google Trends é¦–é ã€‚")
            driver.get(initial_url)
            # ä½¿ç”¨æ›´å¯é çš„ WebDriverWait æ›¿ä»£ time.sleep
            WebDriverWait(driver, 20).until(
                EC.element_to_be_clickable((By.XPATH, rows_xpath))
            )

        except (TimeoutException, NoSuchElementException, StaleElementReferenceException, WebDriverException) as e:
            print(f"âŒ è™•ç†ç„¦é»æ–°è '{keyword}' æ™‚ç™¼ç”Ÿç€è¦½å™¨äº’å‹•éŒ¯èª¤ï¼š{e}")
            print("  > å˜—è©¦é‡æ–°è¼‰å…¥ Google Trends é¦–é ä»¥ç¹¼çºŒ...")
            try:
                driver.get(initial_url)
                WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CLASS_NAME, "mZ3RIc")))
            except Exception as retry_e:
                print(f"  > é‡æ–°è¼‰å…¥å¤±æ•—ï¼Œæ”¾æ£„æ­¤è¼ªè¿´åœˆ: {retry_e}")
            continue # ç¹¼çºŒè™•ç†ä¸‹ä¸€å€‹é—œéµå­—

    return detailed_trends, all_keywords

def send_to_line(detailed_trends, all_keywords):
    """å°‡å…©ä»½å ±å‘Šåˆä½µæˆä¸€æ¢è¨Šæ¯ç™¼é€åˆ° LINE"""
    if not LINE_ACCESS_TOKEN or not LINE_USER_ID:
        print("âŒ ç¼ºå°‘ LINE_ACCESS_TOKEN æˆ– LINE_USER_ID ç’°å¢ƒè®Šæ•¸ï¼Œç„¡æ³•ç™¼é€ã€‚")
        return
    
    if not detailed_trends and not all_keywords:
        print("â„¹ï¸ æ²’æœ‰ä»»ä½•æ•¸æ“šå¯ç™¼é€ã€‚")
        return

    # --- çµ„åˆè¨Šæ¯ ---
    message_parts = []
    
    # ç¬¬ä¸€éƒ¨åˆ†ï¼šè©³ç´°æ–°è
    if detailed_trends:
        message_parts.append("ğŸ“¢ ä»Šæ—¥ç„¦é»æ–°è\n" + "-"*20)
        for i, (keyword, link, title) in enumerate(detailed_trends, 1):
            short_url = shorten_url(link)
            message_parts.append(f"{i}. {keyword}\n  ğŸ“ {title}\n  ğŸ”— {short_url}")
    
    # ç¬¬äºŒéƒ¨åˆ†ï¼šæ‰€æœ‰ç†±é–€é—œéµå­—
    if all_keywords:
        message_parts.append("\nğŸ“Š å…¶ä»–ç†±é–€é—œéµå­—\n" + "-"*20)
        # æ¯è¡Œé¡¯ç¤º 3 å€‹é—œéµå­—ï¼Œè®“è¨Šæ¯æ›´ç·Šæ¹Š
        keyword_lines = []
        line = []
        for keyword in all_keywords:
            line.append(keyword)
            if len(line) == 3:
                keyword_lines.append(" | ".join(line))
                line = []
        if line: # è™•ç†å‰©ä¸‹ä¸æ»¿ 3 å€‹çš„é—œéµå­—
            keyword_lines.append(" | ".join(line))
        message_parts.append("\n".join(keyword_lines))

    full_message = "\n\n".join(message_parts)

    # --- ç™¼é€è¨Šæ¯ ---
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {LINE_ACCESS_TOKEN}"
    }
    # LINE çš„è¨Šæ¯é•·åº¦é™åˆ¶ç‚º 5000 å­—å…ƒï¼Œåšå€‹ä¿éšªæˆªæ–·
    data = {
        "to": LINE_USER_ID,
        "messages": [{"type": "text", "text": full_message[:4999]}]
    }
    try:
        print("æ­£åœ¨ç™¼é€è¨Šæ¯è‡³ LINE...")
        response = requests.post("https://api.line.me/v2/bot/message/push", headers=headers, json=data, timeout=15)
        response.raise_for_status() # å¦‚æœè«‹æ±‚å¤±æ•— (å¦‚ 4xx æˆ– 5xx)ï¼Œæœƒæ‹‹å‡ºç•°å¸¸
        print("âœ… è¨Šæ¯ç™¼é€æˆåŠŸï¼")
    except requests.exceptions.HTTPError as e:
        print(f"âš ï¸ è¨Šæ¯ç™¼é€å¤±æ•—ï¼ŒHTTP éŒ¯èª¤ï¼š{e.response.status_code} {e.response.text}")
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ è¨Šæ¯ç™¼é€å¤±æ•—ï¼Œè«‹æ±‚éŒ¯èª¤ï¼š{e}")

if __name__ == "__main__":
    driver = None  # [é—œéµä¿®æ”¹] å…ˆå°‡ driver åˆå§‹åŒ–ç‚º None
    try:
        print("----- é–‹å§‹åŸ·è¡Œ Google Trends çˆ¬èŸ² -----")
        options = Options()
        options.add_argument("--headless")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage") # åœ¨ Docker/Linux ç’°å¢ƒä¸­éå¸¸é‡è¦
        options.add_argument("--window-size=1920,1080")
        
        print("æ­£åœ¨åˆå§‹åŒ– Chrome WebDriver...")
        driver = webdriver.Chrome(options=options)
        driver.set_page_load_timeout(60) # è¨­å®šé é¢åŠ è¼‰è¶…æ™‚
        print("WebDriver åˆå§‹åŒ–æˆåŠŸã€‚")

        detailed_trends_data, all_keywords_data = get_google_trends_data(driver)

        if detailed_trends_data or all_keywords_data:
            print("\n----- æº–å‚™ç™¼é€è¨Šæ¯è‡³ LINE -----")
            send_to_line(detailed_trends_data, all_keywords_data)
        else:
            print("âš ï¸ ä»Šæ—¥æ²’æœ‰æŠ“å–åˆ°ä»»ä½•ç†±é–€æ•¸æ“šã€‚")

    except WebDriverException as e:
        # ç‰¹åˆ¥è™•ç† WebDriver å•Ÿå‹•å¤±æ•—æˆ–é‹è¡Œæ™‚çš„åš´é‡éŒ¯èª¤
        print(f"âŒ WebDriver åˆå§‹åŒ–å¤±æ•—æˆ–é‹è¡Œæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼š{e}")
        print("è«‹æª¢æŸ¥ Docker ç’°å¢ƒæˆ– Chrome Driver æ˜¯å¦é…ç½®æ­£ç¢ºã€‚")
    except Exception as e:
        # è™•ç†æ‰€æœ‰å…¶ä»–é æœŸå¤–çš„éŒ¯èª¤
        print(f"âŒ ç¨‹å¼åŸ·è¡Œéç¨‹ä¸­ç™¼ç”Ÿè‡´å‘½éŒ¯èª¤ï¼š{e}")
    finally:
        # [é—œéµä¿®æ”¹] ç„¡è«–æˆåŠŸæˆ–å¤±æ•—ï¼Œåªè¦ driver è¢«æˆåŠŸåˆå§‹åŒ–éï¼Œå°±åŸ·è¡Œ quit
        if driver:
            print("\næº–å‚™é—œé–‰ç€è¦½å™¨...")
            driver.quit()
            print("ç€è¦½å™¨å·²é—œé–‰ã€‚")
        print("----- ç¨‹å¼åŸ·è¡Œå®Œç•¢ -----")
