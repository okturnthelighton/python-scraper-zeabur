# å°å…¥æ‰€æœ‰å¿…è¦çš„å‡½å¼åº«
import os
import requests
import json
import urllib.parse
import time
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException, WebDriverException

# --- LINE ç›¸é—œè³‡è¨Š ---
# å¾æ‚¨åœ¨ Zeabur è¨­å®šçš„ç’°å¢ƒè®Šæ•¸ä¸­è®€å–
LINE_ACCESS_TOKEN = os.environ.get("LINE_ACCESS_TOKEN")
LINE_USER_ID = os.environ.get("LINE_USER_ID")
# --- çµæŸ LINE è³‡è¨Šéƒ¨åˆ† ---

def shorten_url(long_url):
    """ä½¿ç”¨ TinyURL API ç¸®çŸ­ç¶²å€"""
    if not long_url or long_url.strip() == "" or long_url == "æ²’æœ‰æ‰¾åˆ°ç›¸é—œé€£çµ":
        return "æ²’æœ‰æ‰¾åˆ°ç›¸é—œé€£çµ"
    api_url = f"https://tinyurl.com/api-create.php?url={urllib.parse.quote(long_url, safe='')}"
    try:
        response = requests.get(api_url, timeout=10)
        return response.text if response.status_code == 200 else long_url
    except requests.exceptions.RequestException:
        return long_url

def get_news_title_from_url(driver, url):
    """
    (ä¾†è‡ªç¬¬ä¸€å€‹è…³æœ¬) è¨ªå•æ–°èé€£çµä¸¦æå–æ¨™é¡Œã€‚
    """
    if not url or url == "æ²’æœ‰æ‰¾åˆ°ç›¸é—œé€£çµ" or url.startswith("https://www.google.com/search?q="):
        return "æ¨™é¡Œè§£æå¤±æ•—æˆ–ç‚ºå‚™ç”¨é€£çµ"
    try:
        print(f"  > æ­£åœ¨è¨ªå•æ–°èé€£çµä»¥ç²å–æ¨™é¡Œ: {url}")
        driver.get(url)
        # ç­‰å¾… title æ¨™ç±¤å‡ºç¾
        title_element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "title")))
        page_title = title_element.get_attribute("textContent").strip()
        if page_title:
            print(f"  > æˆåŠŸå¾ <title> ç²å–æ¨™é¡Œ: {page_title}")
            return page_title
        return "æ¨™é¡Œæœªæ‰¾åˆ°"
    except Exception as e:
        print(f"  > è¨ªå•æ–°èé€£çµæˆ–è§£ææ¨™é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return "æ¨™é¡Œè§£æå¤±æ•—"

def get_google_trends_data(driver):
    """
    åˆä½µå¾Œçš„çˆ¬èŸ²ä¸»å‡½å¼ï¼š
    1. ç²å–åŒ…å«æ–°èæ¨™é¡Œçš„è©³ç´°è¶¨å‹¢ (èˆŠè…³æœ¬åŠŸèƒ½)
    2. ç²å–æ‰€æœ‰ç†±é–€é—œéµå­—çš„åˆ—è¡¨ (æ–°è…³æœ¬åŠŸèƒ½)
    """
    detailed_trends = [] # å„²å­˜ (keyword, link, title)
    all_keywords = [] # å„²å­˜æ‰€æœ‰ keyword

    initial_url = "https://trends.google.com.tw/trending?geo=TW"
    print("æ­£åœ¨è¼‰å…¥ Google Trends é¦–é ...")
    driver.get(initial_url)
    
    try:
        print("ç­‰å¾…ç†±é–€é—œéµå­—åˆ—è¡¨æ¸²æŸ“...")
        WebDriverWait(driver, 30).until(EC.presence_of_all_elements_located((By.CLASS_NAME, "mZ3RIc")))
        print("ç†±é–€é—œéµå­—åˆ—è¡¨å…ƒç´ å·²è¼‰å…¥ã€‚")
    except TimeoutException:
        print("âš ï¸ ç­‰å¾…é¦–é é—œéµå­—åˆ—è¡¨è¶…æ™‚ã€‚")
        return [], []

    # --- é¦–å…ˆï¼ŒåŸ·è¡Œæ–°è…³æœ¬çš„é‚è¼¯ï¼šç²å–æ‰€æœ‰é—œéµå­— ---
    try:
        soup = BeautifulSoup(driver.page_source, "html5lib")
        trend_elements = soup.find_all("div", class_="mZ3RIc")
        if trend_elements:
            print("\n--- æ­£åœ¨æŠ“å–æ‰€æœ‰ç†±é–€é—œéµå­— ---")
            for element in trend_elements:
                keyword = element.text.strip()
                if keyword:
                    all_keywords.append(keyword)
            print(f"æˆåŠŸæŠ“å–åˆ° {len(all_keywords)} å€‹é—œéµå­—ã€‚")
        else:
            print("â„¹ï¸ æœªèƒ½æŠ“å–åˆ°æ‰€æœ‰é—œéµå­—çš„åˆ—è¡¨ã€‚")
    except Exception as e:
        print(f"âŒ æŠ“å–æ‰€æœ‰é—œéµå­—åˆ—è¡¨æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    # --- æ¥è‘—ï¼ŒåŸ·è¡Œç¬¬ä¸€å€‹è…³æœ¬çš„é‚è¼¯ï¼šé»æ“Šæ¯å€‹è¶¨å‹¢ç²å–è©³ç´°æ–°è ---
    print("\n--- æ­£åœ¨é€ä¸€æŠ“å–ç„¦é»æ–°èè©³æƒ… ---")
    rows = driver.find_elements(By.XPATH, '//tr[contains(@class, "UlR2Yc") and @data-row-id]')
    
    for i in range(len(rows)):
        try:
            # æ¯æ¬¡å¾ªç’°éƒ½é‡æ–°æŸ¥æ‰¾å…ƒç´ ï¼Œé¿å… StaleElementReferenceException
            current_rows = driver.find_elements(By.XPATH, '//tr[contains(@class, "UlR2Yc") and @data-row-id]')
            if i >= len(current_rows):
                break # å¦‚æœå…ƒç´ æ•¸é‡è®Šå°‘ï¼Œå‰‡è·³å‡º
            
            row = current_rows[i]
            keyword = row.find_element(By.CLASS_NAME, "mZ3RIc").text.strip()
            print(f"\n- æ­£åœ¨è™•ç†ç„¦é»æ–°è: {keyword}")

            # é»æ“Šé€²å…¥è©³ç´°é é¢
            driver.execute_script("arguments[0].click();", row.find_element(By.CLASS_NAME, "mZ3RIc"))
            
            news_link = "æ²’æœ‰æ‰¾åˆ°ç›¸é—œé€£çµ"
            news_title = "æ¨™é¡Œæœªæ‰¾åˆ°"
            
            try:
                # ç­‰å¾…è©³ç´°é é¢çš„æ–°èé€£çµå‡ºç¾
                news_link_element = WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.XPATH, '//div[@class="jDtQ5"]//a[@class="xZCHj"]')))
                news_link = news_link_element.get_attribute('href')
                print(f"  âœ¨ æ‰¾åˆ°é€£çµ: {news_link}")
                news_title = get_news_title_from_url(driver, news_link)
            except TimeoutException:
                print("  â„¹ï¸ åœ¨è©³ç´°é é¢æ²’æœ‰æ‰¾åˆ°ä¸»è¦æ–°èé€£çµï¼Œå°‡ä½¿ç”¨å‚™ç”¨é€£çµã€‚")
                news_link = f"https://www.google.com/search?q={urllib.parse.quote(keyword)}"
                news_title = "æœå°‹çµæœ (å‚™ç”¨é€£çµ)"

            detailed_trends.append((keyword, news_link, news_title))
            
            # å®Œæˆå¾Œï¼Œå¿…é ˆè¿”å› Google Trends é¦–é 
            print("  > è™•ç†å®Œæˆï¼Œè¿”å› Google Trends é¦–é ã€‚")
            driver.get(initial_url)
            WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CLASS_NAME, "mZ3RIc")))
            time.sleep(1.5)

        except Exception as e:
            print(f"âŒ è™•ç†ç„¦é»æ–°è '{keyword}' æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            print("  > å˜—è©¦é‡æ–°è¼‰å…¥ Google Trends é¦–é ä»¥ç¹¼çºŒ...")
            driver.get(initial_url)
            WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CLASS_NAME, "mZ3RIc")))
            continue

    return detailed_trends, all_keywords

def send_to_line(detailed_trends, all_keywords):
    """å°‡å…©ä»½å ±å‘Šåˆä½µæˆä¸€æ¢è¨Šæ¯ç™¼é€åˆ° LINE"""
    if not LINE_ACCESS_TOKEN or not LINE_USER_ID:
        print("âŒ ç¼ºå°‘ LINE_ACCESS_TOKEN æˆ– LINE_USER_ID ç’°å¢ƒè®Šæ•¸ã€‚")
        return
    
    if not detailed_trends and not all_keywords:
        print("â„¹ï¸ æ²’æœ‰ä»»ä½•æ•¸æ“šå¯ç™¼é€ã€‚")
        return

    # --- çµ„åˆè¨Šæ¯ ---
    message_parts = []
    
    # ç¬¬ä¸€éƒ¨åˆ†ï¼šè©³ç´°æ–°è
    if detailed_trends:
        message_parts.append("ğŸ“¢ ä»Šæ—¥ç„¦é»æ–°è\n" + "-"*20)
        for i, (keyword, link, title) in enumerate(detailed_trends, 1):
            short_url = shorten_url(link)
            message_parts.append(f"{i}. {keyword}\n  ğŸ“ {title}\n  ğŸ”— {short_url}")
    
    # ç¬¬äºŒéƒ¨åˆ†ï¼šæ‰€æœ‰ç†±é–€é—œéµå­—
    if all_keywords:
        message_parts.append("\nğŸ“Š å…¶ä»–ç†±é–€é—œéµå­—\n" + "-"*20)
        keyword_lines = []
        for i, keyword in enumerate(all_keywords, 1):
            keyword_lines.append(f"{i}. {keyword}")
        message_parts.append("\n".join(keyword_lines))

    full_message = "\n".join(message_parts)

    # --- ç™¼é€è¨Šæ¯ ---
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {LINE_ACCESS_TOKEN}"
    }
    # LINE çš„è¨Šæ¯é•·åº¦é™åˆ¶ç‚º 5000 å­—å…ƒï¼Œæˆ‘å€‘çš„å…§å®¹é€šå¸¸ä¸æœƒè¶…é
    data = {
        "to": LINE_USER_ID,
        "messages": [{"type": "text", "text": full_message[:4999]}] # åšå€‹ä¿éšª
    }
    try:
        response = requests.post("https://api.line.me/v2/bot/message/push", headers=headers, json=data)
        response.raise_for_status()
        print("âœ… è¨Šæ¯ç™¼é€æˆåŠŸï¼")
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ è¨Šæ¯ç™¼é€å¤±æ•—ï¼š{e}")

if __name__ == "__main__":
    driver = None
    try:
        print("----- é–‹å§‹åŸ·è¡Œ Google Trends çˆ¬èŸ² -----")
        options = Options()
        options.add_argument("--headless")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--window-size=1920,1080")
        
        driver = webdriver.Chrome(options=options)
        driver.set_page_load_timeout(60)

        detailed_trends_data, all_keywords_data = get_google_trends_data(driver)

        if detailed_trends_data or all_keywords_data:
            print("\n----- æº–å‚™ç™¼é€è¨Šæ¯è‡³ LINE -----")
            send_to_line(detailed_trends_data, all_keywords_data)
        else:
            print("âš ï¸ ä»Šæ—¥æ²’æœ‰æŠ“å–åˆ°ä»»ä½•ç†±é–€æ•¸æ“šã€‚")

    except Exception as e:
        print(f"âŒ ç¨‹å¼åŸ·è¡Œéç¨‹ä¸­ç™¼ç”Ÿè‡´å‘½éŒ¯èª¤ï¼š{e}")
    finally:
        if driver:
            driver.quit()
            print("\nç€è¦½å™¨å·²é—œé–‰ã€‚")
        print("----- ç¨‹å¼åŸ·è¡Œå®Œç•¢ -----")
