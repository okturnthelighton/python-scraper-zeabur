# å°å…¥æ‰€æœ‰å¿…è¦çš„å‡½å¼åº«ï¼Œä¸¦é¡å¤–å°å…¥ os
import os
import requests
import json
import urllib.parse
import time
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException, WebDriverException

# --- LINE ç›¸é—œè³‡è¨Š ---
# å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼Œé€™æ˜¯é›²ç«¯éƒ¨ç½²çš„æœ€ä½³å¯¦è¸
# æ‚¨éœ€è¦åœ¨ Zeabur çš„æœå‹™è¨­å®šä¸­ï¼Œå°‡é€™å…©å€‹å€¼è¨­å®šç‚ºç’°å¢ƒè®Šæ•¸
LINE_ACCESS_TOKEN = os.environ.get("LINE_ACCESS_TOKEN")
LINE_USER_ID = os.environ.get("LINE_USER_ID")
# --- çµæŸ LINE è³‡è¨Šéƒ¨åˆ† ---

def shorten_url(long_url):
    """ä½¿ç”¨ TinyURL API ç¸®çŸ­ç¶²å€"""
    if not long_url or long_url.strip() == "" or long_url == "æ²’æœ‰æ‰¾åˆ°ç›¸é—œé€£çµ":
        return "æ²’æœ‰æ‰¾åˆ°ç›¸é—œé€£çµ"

    api_url = f"https://tinyurl.com/api-create.php?url={urllib.parse.quote(long_url, safe='')}"
    try:
        response = requests.get(api_url, timeout=10)
        if response.status_code == 200:
            return response.text
        else:
            print(f"âš ï¸ ç¸®çŸ­ç¶²å€å¤±æ•—ï¼Œä½¿ç”¨åŸç¶²å€ï¼š{long_url} (HTTP {response.status_code})")
            return long_url
    except requests.exceptions.RequestException as e:
        print(f"âš ï¸ ç¸®çŸ­ç¶²å€æ™‚ç™¼ç”Ÿç¶²è·¯éŒ¯èª¤ï¼š{e}")
        return long_url
    except Exception as e:
        print(f"âš ï¸ ç¸®çŸ­ç¶²å€æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ï¼š{e}")
        return long_url

def get_news_title_from_url(driver, url, initial_url):
    """
    ä½¿ç”¨ Selenium è¨ªå•çµ¦å®š URL ä¸¦å˜—è©¦æå–æ–°èæ¨™é¡Œã€‚
    """
    if not url or url == "æ²’æœ‰æ‰¾åˆ°ç›¸é—œé€£çµ" or url.startswith("https://www.google.com/search?q="):
        return "æ¨™é¡Œè§£æå¤±æ•—æˆ–ç‚ºå‚™ç”¨é€£çµ"

    try:
        print(f"  > æ­£åœ¨è¨ªå•æ–°èé€£çµä»¥ç²å–æ¨™é¡Œ: {url}")
        driver.get(url)
        time.sleep(3)

        # å˜—è©¦å¾ <title> æ¨™ç±¤ç²å–æ¨™é¡Œ
        title_element = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "title"))
        )
        page_title = title_element.get_attribute("textContent").strip()
        if page_title:
            print(f"  > æˆåŠŸå¾ <title> ç²å–æ¨™é¡Œ: {page_title}")
            return page_title
        
        # å¦‚æœ <title> ç‚ºç©ºï¼Œå˜—è©¦å…¶ä»–å¸¸è¦‹å…ƒç´ 
        soup = BeautifulSoup(driver.page_source, "html5lib")
        
        h1_title = soup.find("h1")
        if h1_title and h1_title.get_text(strip=True):
            print(f"  > æˆåŠŸå¾ <h1> ç²å–æ¨™é¡Œ: {h1_title.get_text(strip=True)}")
            return h1_title.get_text(strip=True)

        h2_title = soup.find("h2")
        if h2_title and h2_title.get_text(strip=True):
            print(f"  > æˆåŠŸå¾ <h2> ç²å–æ¨™é¡Œ: {h2_title.get_text(strip=True)}")
            return h2_title.get_text(strip=True)
            
        common_title_selectors = [
            {'name': 'div', 'class_': 'title'},
            {'name': 'span', 'class_': 'title'},
            {'name': 'div', 'id': 'news_title'},
            {'name': 'div', 'class_': 'article-title'},
            {'name': 'h1', 'class_': 'article-title'},
        ]
        
        for selector in common_title_selectors:
            if 'class_' in selector:
                found_title = soup.find(selector['name'], class_=selector['class_'])
            elif 'id' in selector:
                found_title = soup.find(selector['name'], id=selector['id'])
            
            if found_title and found_title.get_text(strip=True):
                print(f"  > æˆåŠŸå¾ {selector['name']}.{selector.get('class_', selector.get('id'))} ç²å–æ¨™é¡Œ: {found_title.get_text(strip=True)}")
                return found_title.get_text(strip=True)

        print("  > æœªèƒ½å¾æ–°èé€£çµé é¢æå–åˆ°åˆé©çš„æ¨™é¡Œã€‚")
        return "æ¨™é¡Œæœªæ‰¾åˆ°"

    except (TimeoutException, NoSuchElementException, WebDriverException) as e:
        print(f"  > è¨ªå•æ–°èé€£çµæˆ–è§£ææ¨™é¡Œæ™‚ç™¼ç”Ÿ Selenium éŒ¯èª¤ï¼š{e}")
        return "æ¨™é¡Œè§£æå¤±æ•—"
    except Exception as e:
        print(f"  > è¨ªå•æ–°èé€£çµæˆ–è§£ææ¨™é¡Œæ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ï¼š{e}")
        return "æ¨™é¡Œè§£æå¤±æ•—"
    # æ³¨æ„ï¼šè¿”å› Google Trends çš„é‚è¼¯å·²ç§»è‡³ä¸»å¾ªç’°ä¸­ï¼Œä»¥ç¢ºä¿æ¯æ¬¡éƒ½èƒ½æ­£ç¢ºè¿”å›

def get_google_trends_with_selenium():
    """ä½¿ç”¨ Selenium çˆ¬å– Google Trends ç†±é–€é—œéµå­—ã€æ–°èé€£çµåŠæ–°èæ¨™é¡Œ"""
    chrome = None
    hot_trends_data = []
    initial_url = "https://trends.google.com.tw/trending?geo=TW"

    try:
        options = Options()
        # åœ¨ Docker ç’°å¢ƒä¸­ï¼Œå¿…é ˆä½¿ç”¨ headless æ¨¡å¼
        options.add_argument("--headless")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox") # åœ¨ Docker ä¸­åŸ·è¡Œ Chrome çš„å¿…è¦åƒæ•¸
        options.add_argument("--disable-dev-shm-usage") # å…‹æœæœ‰é™çš„è³‡æºå•é¡Œ
        options.add_argument("--window-size=1920,1080")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("--log-level=3")

        # ã€é—œéµä¿®æ”¹ã€‘ç§»é™¤ Service ç‰©ä»¶ï¼ŒSelenium 4 æœƒè‡ªå‹•ç®¡ç†é©…å‹•ç¨‹å¼
        chrome = webdriver.Chrome(options=options)
        chrome.set_page_load_timeout(60)

        print("æ­£åœ¨è¼‰å…¥ Google Trends é¦–é ...")
        chrome.get(initial_url)
        print("é¦–é è¼‰å…¥å®Œæˆï¼Œæ­£åœ¨ç­‰å¾…ç†±é–€é—œéµå­—åˆ—è¡¨æ¸²æŸ“...")

        try:
            WebDriverWait(chrome, 30).until(
                EC.presence_of_all_elements_located((By.CLASS_NAME, "mZ3RIc"))
            )
            print("ç†±é–€é—œéµå­—åˆ—è¡¨å…ƒç´ å·²è¼‰å…¥ã€‚")
        except TimeoutException:
            print("âš ï¸ ç­‰å¾…é¦–é é—œéµå­—åˆ—è¡¨è¶…æ™‚ã€‚")
            return []

        rows = chrome.find_elements(By.XPATH, '//tr[contains(@class, "UlR2Yc") and @data-row-id]')
        
        trend_elements_on_homepage = []
        for row in rows:
            try:
                row_id = int(row.get_attribute("data-row-id"))
                keyword_div = row.find_element(By.CLASS_NAME, "mZ3RIc")
                keyword = keyword_div.text.strip()
                if keyword:
                    trend_elements_on_homepage.append({
                        "row_id": row_id,
                        "keyword": keyword,
                        "element_to_click_xpath": f'//tr[@data-row-id="{row_id}"]//div[@class="mZ3RIc"]'
                    })
            except Exception as e:
                print(f"âš ï¸ é¦–é è§£æå–®ä¸€è¶¨å‹¢é …ç›®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue

        print(f"é¦–é å…±ç™¼ç¾ {len(trend_elements_on_homepage)} å€‹ç†±é–€é—œéµå­—ã€‚")

        for trend_info in trend_elements_on_homepage:
            keyword = trend_info["keyword"]
            row_id = trend_info["row_id"]
            element_xpath = trend_info["element_to_click_xpath"]
            
            print(f"\n--- æ­£åœ¨è™•ç†é—œéµå­—: [{row_id}] {keyword} ---")
            
            try:
                clickable_element = WebDriverWait(chrome, 15).until(
                    EC.element_to_be_clickable((By.XPATH, element_xpath))
                )
                chrome.execute_script("arguments[0].click();", clickable_element)
                print(f"æˆåŠŸé»æ“Šé—œéµå­— '{keyword}'ï¼Œæ­£åœ¨è¼‰å…¥è©³ç´°é é¢...")
                
                WebDriverWait(chrome, 25).until(
                    EC.presence_of_element_located((By.XPATH, '//div[@class="jDtQ5"]//a[@class="xZCHj"]'))
                )
                print(f"è©³ç´°é é¢æ–°èé€£çµå…ƒç´ å·²è¼‰å…¥ã€‚")
                time.sleep(1)

                detail_soup = BeautifulSoup(chrome.page_source, "html5lib")
                news_content_div = detail_soup.find("div", class_="jDtQ5")
                news_link = "æ²’æœ‰æ‰¾åˆ°ç›¸é—œé€£çµ"
                news_title = "æ¨™é¡Œæœªæ‰¾åˆ°"

                if news_content_div and news_content_div.find("a", class_="xZCHj"):
                    news_link = news_content_div.find("a", class_="xZCHj")['href']
                    print(f"âœ¨ æ‰¾åˆ°é€£çµ: {news_link}")
                    # ç²å–æ¨™é¡Œ
                    news_title = get_news_title_from_url(chrome, news_link, initial_url)
                else:
                    print("â„¹ï¸ åœ¨è©³ç´°é é¢æ²’æœ‰æ‰¾åˆ°ä¸»è¦æ–°èé€£çµï¼Œå°‡ä½¿ç”¨å‚™ç”¨é€£çµã€‚")
                    news_link = f"https://www.google.com/search?q={urllib.parse.quote(keyword)}"
                    news_title = "æœå°‹çµæœ (å‚™ç”¨é€£çµ)"

                hot_trends_data.append((keyword, news_link, news_title))
                
                # å®Œæˆä¸€å€‹é—œéµå­—è™•ç†å¾Œï¼Œå¿…é ˆè¿”å› Google Trends é¦–é 
                print("  > è™•ç†å®Œæˆï¼Œè¿”å› Google Trends é¦–é ä»¥ç¹¼çºŒã€‚")
                chrome.get(initial_url)
                WebDriverWait(chrome, 15).until(
                    EC.presence_of_all_elements_located((By.CLASS_NAME, "mZ3RIc"))
                )
                time.sleep(1.5)

            except Exception as e:
                print(f"âŒ è™•ç†é—œéµå­— '{keyword}' æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                print("å˜—è©¦é‡æ–°è¼‰å…¥ Google Trends é¦–é ä»¥ç¹¼çºŒ...")
                try:
                    chrome.get(initial_url)
                    WebDriverWait(chrome, 20).until(
                        EC.presence_of_all_elements_located((By.CLASS_NAME, "mZ3RIc"))
                    )
                    print("æˆåŠŸé‡æ–°è¼‰å…¥ Google Trends é¦–é ã€‚")
                    time.sleep(2)
                except Exception as re_e:
                    print(f"âš ï¸ ç„¡æ³•é‡æ–°è¼‰å…¥ Google Trends é¦–é ï¼Œç¨‹å¼å¯èƒ½ç„¡æ³•ç¹¼çºŒï¼š{re_e}")
                    break # å¦‚æœç„¡æ³•æ¢å¾©ï¼Œå‰‡ä¸­æ–·å¾ªç’°
                continue

        if hot_trends_data:
            print(f"\nç¸½å…±æˆåŠŸæŠ“å–åˆ° {len(hot_trends_data)} å€‹è¶¨å‹¢ã€‚")
        else:
            print("æ²’æœ‰æ‰¾åˆ°ä»»ä½•è¶¨å‹¢ã€‚")

    except Exception as e:
        print(f"âŒ çˆ¬èŸ²éç¨‹ä¸­ç™¼ç”Ÿè‡´å‘½éŒ¯èª¤ï¼š{e}")
    finally:
        if chrome is not None:
            chrome.quit()
            print("\nç€è¦½å™¨å·²é—œé–‰ã€‚")
    
    return hot_trends_data

def send_to_line(hot_trends_data):
    """å°‡ç†±é–€é—œéµå­—ã€é€£çµåŠæ¨™é¡Œç™¼é€åˆ° LINE"""
    if not LINE_ACCESS_TOKEN or not LINE_USER_ID:
        print("âŒ ç¼ºå°‘ LINE_ACCESS_TOKEN æˆ– LINE_USER_ID ç’°å¢ƒè®Šæ•¸ï¼Œç„¡æ³•ç™¼é€è¨Šæ¯ã€‚")
        return
    
    if not hot_trends_data:
        print("â„¹ï¸ æ²’æœ‰é—œéµå­—æ•¸æ“šå¯ç™¼é€ï¼Œè·³é LINE è¨Šæ¯ã€‚")
        return

    message_header = "ğŸ“¢ ä»Šæ—¥ Google Trends ç†±é–€é—œéµå­—åŠæ–°è:\n"
    current_message_part = message_header
    
    for i, (keyword, original_link, news_title) in enumerate(hot_trends_data, 1):
        short_url = shorten_url(original_link)
        new_line = f"{i}. {keyword}\n  ğŸ“ {news_title}\n  ğŸ”— {short_url}\n"
        
        # æª¢æŸ¥åŠ ä¸Šæ–°å…§å®¹å¾Œæ˜¯å¦æœƒè¶…é LINE çš„å–®æ¬¡è¨Šæ¯é•·åº¦é™åˆ¶ (ç´„ 5000 å­—å…ƒ)
        if len(current_message_part.encode('utf-8')) + len(new_line.encode('utf-8')) > 4800:
            # ç™¼é€ç•¶å‰ç´¯ç©çš„è¨Šæ¯
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {LINE_ACCESS_TOKEN}"
            }
            data = {
                "to": LINE_USER_ID,
                "messages": [{"type": "text", "text": current_message_part.strip()}]
            }
            try:
                response = requests.post("https://api.line.me/v2/bot/message/push", headers=headers, json=data)
                response.raise_for_status()
                print(f"âœ… è¨Šæ¯åˆ†æ®µç™¼é€æˆåŠŸï¼")
                time.sleep(1)
                current_message_part = "ğŸ“¢ (çºŒ) Google Trends ç†±é–€é—œéµå­—åŠæ–°è:\n"
            except requests.exceptions.RequestException as e:
                print(f"âš ï¸ è¨Šæ¯åˆ†æ®µç™¼é€å¤±æ•—ï¼š{e}")
                current_message_part = "ğŸ“¢ (çºŒ) Google Trends ç†±é–€é—œéµå­—åŠæ–°è:\n"
        
        current_message_part += new_line

    # ç™¼é€æœ€å¾Œå‰©é¤˜çš„è¨Šæ¯
    if current_message_part.strip() != message_header.strip() and \
       current_message_part.strip() != "ğŸ“¢ (çºŒ) Google Trends ç†±é–€é—œéµå­—åŠæ–°è:":
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {LINE_ACCESS_TOKEN}"
        }
        data = {
            "to": LINE_USER_ID,
            "messages": [{"type": "text", "text": current_message_part.strip()}]
        }
        try:
            response = requests.post("https://api.line.me/v2/bot/message/push", headers=headers, json=data)
            response.raise_for_status()
            print("âœ… æœ€å¾Œä¸€æ®µè¨Šæ¯ç™¼é€æˆåŠŸï¼")
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ æœ€å¾Œä¸€æ®µè¨Šæ¯ç™¼é€å¤±æ•—ï¼š{e}")

if __name__ == "__main__":
    print("----- é–‹å§‹åŸ·è¡Œ Google Trends çˆ¬èŸ²èˆ‡ LINE æ¨æ’­ -----")
    hot_trends_data = get_google_trends_with_selenium()

    if hot_trends_data:
        print("\n----- æº–å‚™ç™¼é€è¨Šæ¯è‡³ LINE -----")
        send_to_line(hot_trends_data)
    else:
        print("âš ï¸ ä»Šæ—¥æ²’æœ‰ç†±é–€æœå°‹æ•¸æ“šï¼Œä¸ç™¼é€ LINE è¨Šæ¯ã€‚")
    print("----- ç¨‹å¼åŸ·è¡Œå®Œç•¢ -----")
